docker_prometheus:
  apps:
    - name: prometheus-stack
      location: /opt/prometheus-stack

  prometheus-stack:
    limit_nofile: 65536

    cmd:
      start: docker-compose up
      stop: docker-compose stop

    supporting_files:
      - name: prometheus.yml
        mode: '744'
        context: |
          {%- raw %}
          # my global config
          global:
            scrape_interval:     15s # By default, scrape targets every 15 seconds.
            evaluation_interval: 15s # By default, scrape targets every 15 seconds.
            # scrape_timeout is set to the global default (10s).

            # Attach these labels to any time series or alerts when communicating with
            # external systems (federation, remote storage, Alertmanager).
            external_labels:
                monitor: 'my-project'

          # Load and evaluate rules in this file every 'evaluation_interval' seconds.
          rule_files:
            - 'alert.rules'
            # - "first.rules"
            # - "second.rules"

          # alert
          alerting:
            alertmanagers:
            - scheme: http
              static_configs:
              - targets:
                - "alertmanager:9093"

          # A scrape configuration containing exactly one endpoint to scrape:
          # Here it's Prometheus itself.
          scrape_configs:
            # The job name is added as a label `job=<job_name>` to any timeseries scraped from this config.

            - job_name: 'prometheus'

              # Override the global default and scrape targets from this job every 5 seconds.
              scrape_interval: 5s

              static_configs:
                  - targets: ['localhost:9090']

            - job_name: 'cadvisor'

              # Override the global default and scrape targets from this job every 5 seconds.
              scrape_interval: 5s

              dns_sd_configs:
              - names:
                - 'tasks.cadvisor'
                type: 'A'
                port: 8080

          #     static_configs:
          #          - targets: ['cadvisor:8080']

            - job_name: 'node-exporter'

              # Override the global default and scrape targets from this job every 5 seconds.
              scrape_interval: 5s

              dns_sd_configs:
              - names:
                - 'tasks.node-exporter'
                type: 'A'
                port: 9100

            - job_name: 'pushgateway'
              scrape_interval: 10s
              dns_sd_configs:
              - names:
                - 'tasks.pushgateway'
                type: 'A'
                port: 9091
                static_configs:
                 - targets: ['node-exporter:9100']
          {%- endraw %}

      - name: prometheus-alert.rules
        mode: '744'
        context: |
          {%- raw %}
          groups:
          - name: example
            rules:

            # Alert for any instance that is unreachable for >2 minutes.
            - alert: service_down
              expr: up == 0
              for: 2m
              labels:
                severity: page
              annotations:
                summary: "Instance {{ $labels.instance }} down"
                description: "{{ $labels.instance }} of job {{ $labels.job }} has been down for more than 2 minutes."

            - alert: high_load
              expr: node_load1 > 0.5
              for: 2m
              labels:
                severity: page
              annotations:
                summary: "Instance {{ $labels.instance }} under high load"
                description: "{{ $labels.instance }} of job {{ $labels.job }} is under high load."
          {%- endraw %}
      - name: alertmanager-config.yml
        mode: '744'
        contents: |
          {%- raw %}
          global:
            # The smarthost and SMTP sender used for mail notifications.
            smtp_smarthost: 'localhost:25'
            smtp_from: 'alertmanager@example.org'
            smtp_auth_username: 'alertmanager'
            smtp_auth_password: 'password'

          # The directory from which notification templates are read.
          templates: 
          - '/etc/alertmanager/template/*.tmpl'

          # The root route on which each incoming alert enters.
          route:
            # The labels by which incoming alerts are grouped together. For example,
            # multiple alerts coming in for cluster=A and alertname=LatencyHigh would
            # be batched into a single group.
            #
            # To aggregate by all possible labels use '...' as the sole label name.
            # This effectively disables aggregation entirely, passing through all
            # alerts as-is. This is unlikely to be what you want, unless you have
            # a very low alert volume or your upstream notification system performs
            # its own grouping. Example: group_by: [...]
            group_by: ['alertname', 'cluster', 'service']

            # When a new group of alerts is created by an incoming alert, wait at
            # least 'group_wait' to send the initial notification.
            # This way ensures that you get multiple alerts for the same group that start
            # firing shortly after another are batched together on the first 
            # notification.
            group_wait: 30s

            # When the first notification was sent, wait 'group_interval' to send a batch
            # of new alerts that started firing for that group.
            group_interval: 5m

            # If an alert has successfully been sent, wait 'repeat_interval' to
            # resend them.
            repeat_interval: 3h 

            # A default receiver
            receiver: team-X-mails

            # All the above attributes are inherited by all child routes and can 
            # overwritten on each.

            # The child route trees.
            routes:
            # This routes performs a regular expression match on alert labels to
            # catch alerts that are related to a list of services.
            - match_re:
                service: ^(foo1|foo2|baz)$
              receiver: team-X-mails
              # The service has a sub-route for critical alerts, any alerts
              # that do not match, i.e. severity != critical, fall-back to the
              # parent node and are sent to 'team-X-mails'
              routes:
              - match:
                  severity: critical
                receiver: team-X-pager
            - match:
                service: files
              receiver: team-Y-mails

              routes:
              - match:
                  severity: critical
                receiver: team-Y-pager

            # This route handles all alerts coming from a database service. If there's
            # no team to handle it, it defaults to the DB team.
            - match:
                service: database
              receiver: team-DB-pager
              # Also group alerts by affected database.
              group_by: [alertname, cluster, database]
              routes:
              - match:
                  owner: team-X
                receiver: team-X-pager
                continue: true
              - match:
                  owner: team-Y
                receiver: team-Y-pager


          # Inhibition rules allow to mute a set of alerts given that another alert is
          # firing.
          # We use this to mute any warning-level notifications if the same alert is 
          # already critical.
          inhibit_rules:
          - source_match:
              severity: 'critical'
            target_match:
              severity: 'warning'
            # Apply inhibition if the alertname is the same.
            # CAUTION: 
            #   If all label names listed in `equal` are missing 
            #   from both the source and target alerts,
            #   the inhibition rule will apply!
            equal: ['alertname', 'cluster', 'service']


          receivers:
          - name: 'team-X-mails'
            email_configs:
            - to: 'team-X+alerts@example.org'

          - name: 'team-X-pager'
            email_configs:
            - to: 'team-X+alerts-critical@example.org'
            pagerduty_configs:
            - service_key: <team-X-key>

          - name: 'team-Y-mails'
            email_configs:
            - to: 'team-Y+alerts@example.org'

          - name: 'team-Y-pager'
            pagerduty_configs:
            - service_key: <team-Y-key>

          - name: 'team-DB-pager'
            pagerduty_configs:
            - service_key: <team-DB-key>
          {%- endraw %}

    env:
      GF_SECURITY_ADMIN_PASSWORD: 'password1234'
      GF_USERS_ALLOW_SIGN_UP: false

    docker_compose_yaml: |
      version: '3.7'

      volumes:
          prometheus_data: {}
          grafana_data: {}

      networks:
        front-tier:
        prom-back-tier:

      services:

        prometheus:
          image: prom/prometheus:v2.1.0
          volumes:
            - ./prometheus.yml:/etc/prometheus/prometheus.yml:ro
            - ./alert.rules:/etc/prometheus/alert.rules:ro
            - prometheus_data:/prometheus
          command:
            - '--config.file=/etc/prometheus/prometheus.yml'
            - '--storage.tsdb.path=/prometheus'
            - '--web.console.libraries=/usr/share/prometheus/console_libraries'
            - '--web.console.templates=/usr/share/prometheus/consoles'
          ports:
            - 9090:9090
          links:
            - cadvisor:cadvisor
            - alertmanager:alertmanager
            - pushgateway:pushgateway
          depends_on:
            - cadvisor
            - pushgateway
          networks:
            - prom-back-tier
          restart: always
      #    deploy:
      #      placement:
      #        constraints:
      #          - node.hostname == ${HOSTNAME}

        node-exporter:
          image: prom/node-exporter
          volumes:
            - /proc:/host/proc:ro
            - /sys:/host/sys:ro
            - /:/rootfs:ro
          command: 
            - '--path.procfs=/host/proc' 
            - '--path.sysfs=/host/sys'
            - --collector.filesystem.ignored-mount-points
            - "^/(sys|proc|dev|host|etc|rootfs/var/lib/docker/containers|rootfs/var/lib/docker/overlay2|rootfs/run/docker/netns|rootfs/var/lib/docker/aufs)($$|/)"
          ports:
            - 9100:9100
          networks:
            - prom-back-tier
          restart: always
          deploy:
            mode: global

        alertmanager:
          image: prom/alertmanager
          ports:
            - 9093:9093
          volumes:
            - ./alertmanager-config.yml:/etc/alertmanager/config.yml
          networks:
            - prom-back-tier
          restart: always
          command:
            - '--config.file=/etc/alertmanager/config.yml'
            - '--storage.path=/alertmanager'
      #    deploy:
      #      placement:
      #        constraints:
      #          - node.hostname == ${HOSTNAME}

        cadvisor:
          image: google/cadvisor
          volumes:
            - /:/rootfs:ro
            - /var/run:/var/run:rw
            - /sys:/sys:ro
            - /var/lib/docker/:/var/lib/docker:ro
          ports:
            - 8080:8080
          networks:
            - prom-back-tier
          restart: always
          deploy:
            mode: global

        grafana:
          image: grafana/grafana
          user: "472"
          depends_on:
            - prometheus
          ports:
            - 3010:3000
          volumes:
            - grafana_data:/var/lib/grafana
            - ./grafana/provisioning/:/etc/grafana/provisioning/
          env_file:
            - ./.env
          networks:
            - prom-back-tier
            - front-tier
          restart: always

        pushgateway:
          image: prom/pushgateway
          restart: always
          expose:
            - 9091
          ports:
            - "9091:9091"
          networks:
            - prom-back-tier